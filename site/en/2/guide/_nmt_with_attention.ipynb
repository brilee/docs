{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AOpGoE2T-YXS"
   },
   "source": [
    "##### Copyright 2018 The TensorFlow Authors.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\").\n",
    "\n",
    "# Neural Machine Translation with Attention\n",
    "\n",
    "<table class=\"tfo-notebook-buttons\" align=\"left\"><td>\n",
    "<a target=\"_blank\"  href=\"https://colab.research.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/contrib/eager/python/examples/nmt_with_attention/nmt_with_attention.ipynb\">\n",
    "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>  \n",
    "</td><td>\n",
    "<a target=\"_blank\"  href=\"https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/eager/python/examples/nmt_with_attention/nmt_with_attention.ipynb\"><img width=32px src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a></td></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CiwtNgENbx2g"
   },
   "source": [
    "This notebook trains a sequence to sequence (seq2seq) model for Spanish to English translation using TF 2.0 APIs. This is an advanced example that assumes some knowledge of sequence to sequence models.\n",
    "\n",
    "After training the model in this notebook, you will be able to input a Spanish sentence, such as *\"¿todavia estan en casa?\"*, and return the English translation: *\"are you still at home?\"*\n",
    "\n",
    "The translation quality is reasonable for a toy example, but the generated attention plot is perhaps more interesting. This shows which parts of the input sentence has the model's attention while translating:\n",
    "\n",
    "<img src=\"https://tensorflow.org/images/spanish-english.png\" alt=\"spanish-english attention plot\">\n",
    "\n",
    "Note: This example takes approximately 10 mintues to run on a single P100 GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tf-nightly-2.0-preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tnxXKDjq3jEL"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import collections\n",
    "import itertools\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import time\n",
    "import unicodedata\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "# TODO(brianklee): remove when summaryv2 ops are exported\n",
    "\n",
    "from tensorflow.python.ops import summary_ops_v2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wfodePkj3jEa"
   },
   "source": [
    "## Download and prepare the dataset\n",
    "\n",
    "We'll use a language dataset provided by http://www.manythings.org/anki/. This dataset contains language translation pairs in the format:\n",
    "\n",
    "```\n",
    "May I borrow this book?\t¿Puedo tomar prestado este libro?\n",
    "```\n",
    "\n",
    "There are a variety of languages available, but we'll use the English-Spanish dataset. For convenience, we've hosted a copy of this dataset on Google Cloud, but you can also download your own copy. After downloading the dataset, here are the steps we'll take to prepare the data:\n",
    "\n",
    "1. Add a *start* and *end* token to each sentence.\n",
    "2. Clean the sentences by removing special characters.\n",
    "3. Create a word index and reverse word index (dictionaries mapping from word → id and id → word).\n",
    "4. Pad each sentence to a maximum length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kRVATYOgJs1b"
   },
   "outputs": [],
   "source": [
    "class LanguageIndex(object):\n",
    "  \"\"\"A bidirectional mapping from word <=> integer index.\"\"\"\n",
    "\n",
    "  def __init__(self, vocab):\n",
    "    self.word2idx = collections.defaultdict(int)  # If not in vocab, return 0.\n",
    "    self.idx2word = {}\n",
    "    for i, word in enumerate(vocab):\n",
    "      self.word2idx[word] = i + 1\n",
    "      self.idx2word[i + 1] = word\n",
    "    self.idx2word[0] = '<OOV>'  # \"Out of Vocab\"\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.idx2word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rd0jw-eC3jEh"
   },
   "outputs": [],
   "source": [
    "# Converts the unicode file to ascii\n",
    "# https://stackoverflow.com/a/518232/2809427\n",
    "def unicode_to_ascii(s):\n",
    "  return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "                 if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "START_TOKEN = '<start>'\n",
    "END_TOKEN = '<end>'\n",
    "\n",
    "def preprocess_sentence(w):\n",
    "  w = unicode_to_ascii(w.lower().strip())\n",
    "\n",
    "  # creating a space between a word and the punctuation following it\n",
    "  # eg: \"he is a boy.\" => \"he is a boy .\"\n",
    "  # https://stackoverflow.com/a/3645931/3645946\n",
    "  w = re.sub(r'([?.!,¿])', r' \\1 ', w)\n",
    "\n",
    "  # replacing everything with space except (a-z, A-Z, '.', '?', '!', ',')\n",
    "  w = re.sub(r'[^a-zA-Z?.!,¿]+', ' ', w)\n",
    "\n",
    "  # adding a start and an end token to the sentence\n",
    "  # so that the model know when to start and stop predicting.\n",
    "  return [START_TOKEN] + w.split() + [END_TOKEN]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training on the complete dataset of >100,000 sentences will take a long time. To train faster, we can limit the size of the dataset (of course, translation quality degrades with less data).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OHn4Dct23jEm"
   },
   "outputs": [],
   "source": [
    "# TODO(brianklee): This preprocessing should ideally be implemented in TF\n",
    "# because preprocessing should be exported as part of the SavedModel.\n",
    "# In other words, the NmtTranslator's interface should take a list of strings,\n",
    "# not a list of integers.\n",
    "def load_anki_data(num_examples=None):\n",
    "  # Download the file\n",
    "  path_to_zip = tf.keras.utils.get_file(\n",
    "      'spa-eng.zip', origin='http://download.tensorflow.org/data/spa-eng.zip',\n",
    "      extract=True)\n",
    "\n",
    "  path_to_file = os.path.dirname(path_to_zip) + '/spa-eng/spa.txt'\n",
    "  with open(path_to_file, 'rb') as f:\n",
    "    lines = f.read().decode('utf8').strip().split('\\n')\n",
    "  # Clean the sentences\n",
    "  eng_spa_pairs = [line.split('\\t') for line in lines]\n",
    "  eng_spa_pairs = [(preprocess_sentence(eng), preprocess_sentence(spa))\n",
    "                   for eng, spa in eng_spa_pairs]\n",
    "  # The translations file is ordered from shortest to longest, so slicing from\n",
    "  # the front will select the shorter examples. This speeds up training.\n",
    "  if num_examples is not None:\n",
    "    eng_spa_pairs = eng_spa_pairs[:num_examples]\n",
    "  eng_sentences, spa_sentences = zip(*eng_spa_pairs)\n",
    "  # Construct a vocabulary and integer mapping for both Spanish/English.\n",
    "  eng_vocab = sorted(set(itertools.chain.from_iterable(eng_sentences)))\n",
    "  spa_vocab = sorted(set(itertools.chain.from_iterable(spa_sentences)))\n",
    "  eng_index = LanguageIndex(eng_vocab)\n",
    "  spa_index = LanguageIndex(spa_vocab)\n",
    "  return eng_spa_pairs, eng_index, spa_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EXAMPLES = 30000\n",
    "sentence_pairs, english_index, spanish_index = load_anki_data(NUM_EXAMPLES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eAY9k49G3jE_"
   },
   "outputs": [],
   "source": [
    "# Turn our english/spanish pairs into TF Datasets by mapping words -> integers.\n",
    "def make_dataset(eng_spa_pairs, eng_index, spa_index):\n",
    "  eng_sentences, spa_sentences = zip(*eng_spa_pairs)\n",
    "  eng_ints = [[eng_index.word2idx[word] for word in sentence] for sentence in eng_sentences]\n",
    "  spa_ints = [[spa_index.word2idx[word] for word in sentence] for sentence in spa_sentences]\n",
    "\n",
    "  max_eng_len = max(map(len, eng_sentences))\n",
    "  max_spa_len = max(map(len, spa_sentences))\n",
    "  padded_eng_ints = tf.keras.preprocessing.sequence.pad_sequences(eng_ints, maxlen=max_eng_len, padding='post')\n",
    "  padded_spa_ints = tf.keras.preprocessing.sequence.pad_sequences(spa_ints, maxlen=max_spa_len, padding='post')\n",
    "\n",
    "  dataset = tf.data.Dataset.from_tensor_slices((padded_eng_ints, padded_spa_ints))\n",
    "  return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test split\n",
    "train_size = int(len(sentence_pairs) * 0.8)\n",
    "random.shuffle(sentence_pairs)\n",
    "train_sentence_pairs, test_sentence_pairs = sentence_pairs[:train_size], sentence_pairs[train_size:]\n",
    "# Show length\n",
    "len(train_sentence_pairs), len(test_sentence_pairs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentence_pairs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4QILQkOs3jFG"
   },
   "outputs": [],
   "source": [
    "# Set up datasets\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_ds = make_dataset(train_sentence_pairs, english_index, spanish_index)\n",
    "test_ds = make_dataset(test_sentence_pairs, english_index, spanish_index)\n",
    "train_ds = train_ds.shuffle(len(train_sentence_pairs)).batch(BATCH_SIZE, drop_remainder=True)\n",
    "test_ds = test_ds.batch(BATCH_SIZE, drop_remainder=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_batch = next(iter(train_ds))\n",
    "print(\"Dataset outputs elements with shape ({}, {})\".format(first_batch[0].shape, first_batch[1].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TNfHIF71ulLu"
   },
   "source": [
    "## Write the encoder and decoder model\n",
    "\n",
    "Here, we'll implement an encoder-decoder model with attention which you can read about in the TensorFlow [Neural Machine Translation (seq2seq) tutorial](https://www.tensorflow.org/tutorials/seq2seq). This example uses a more recent set of APIs. This notebook implements the [attention equations](https://www.tensorflow.org/tutorials/seq2seq#background_on_the_attention_mechanism) from the seq2seq tutorial. The following diagram shows that each input words is assigned a weight by the attention mechanism which is then used by the decoder to predict the next word in the sentence.\n",
    "\n",
    "<img src=\"https://www.tensorflow.org/images/seq2seq/attention_mechanism.jpg\" width=\"500\" alt=\"attention mechanism\">\n",
    "\n",
    "The input is put through an encoder model which gives us the encoder output of shape *(batch_size, max_length, hidden_size)* and the encoder hidden state of shape *(batch_size, hidden_size)*. \n",
    "\n",
    "Here are the equations that are implemented:\n",
    "\n",
    "<img src=\"https://www.tensorflow.org/images/seq2seq/attention_equation_0.jpg\" alt=\"attention equation 0\" width=\"800\">\n",
    "<img src=\"https://www.tensorflow.org/images/seq2seq/attention_equation_1.jpg\" alt=\"attention equation 1\" width=\"800\">\n",
    "\n",
    "We're using *Bahdanau attention*. Lets decide on notation before writing the simplified form:\n",
    "\n",
    "* FC = Fully connected (dense) layer\n",
    "* EO = Encoder output\n",
    "* H = hidden state\n",
    "* X = input to the decoder\n",
    "\n",
    "And the pseudo-code:\n",
    "\n",
    "* `score = FC(tanh(FC(EO) + FC(H)))`\n",
    "* `attention weights = softmax(score, axis = 1)`. Softmax by default is applied on the last axis but here we want to apply it on the *1st axis*, since the shape of score is *(batch_size, max_length, hidden_size)*. `Max_length` is the length of our input. Since we are trying to assign a weight to each input, softmax should be applied on that axis.\n",
    "* `context vector = sum(attention weights * EO, axis = 1)`. Same reason as above for choosing axis as 1.\n",
    "* `embedding output` = The input to the decoder X is passed through an embedding layer.\n",
    "* `merged vector = concat(embedding output, context vector)`\n",
    "* This merged vector is then given to the GRU\n",
    "  \n",
    "The shapes of all the vectors at each step have been specified in the comments in the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "avyJ_4VIUoHb"
   },
   "outputs": [],
   "source": [
    "def gru(units):\n",
    "  # If you have a GPU, we recommend using CuDNNGRU(provides a 3x speedup than GRU)\n",
    "  # the code automatically does that.\n",
    "  if tf.test.is_gpu_available():\n",
    "    return tf.keras.layers.CuDNNGRU(units,\n",
    "                                    return_sequences=True,\n",
    "                                    return_state=True,\n",
    "                                    recurrent_initializer='glorot_uniform')\n",
    "  else:\n",
    "    return tf.keras.layers.GRU(units,\n",
    "                               return_sequences=True,\n",
    "                               return_state=True,\n",
    "                               recurrent_activation='sigmoid',\n",
    "                               recurrent_initializer='glorot_uniform')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENCODER_SIZE = DECODER_SIZE = 128\n",
    "EMBEDDING_DIM = 32\n",
    "MAX_OUTPUT_LENGTH = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nZ2rI24i3jFg"
   },
   "outputs": [],
   "source": [
    "\n",
    "class Encoder(tf.keras.Model):\n",
    "  def __init__(self, vocab_size):\n",
    "    super(Encoder, self).__init__()\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, EMBEDDING_DIM)\n",
    "    self.gru = gru(ENCODER_SIZE)\n",
    "\n",
    "  def call(self, x, hidden):\n",
    "    x = self.embedding(x)\n",
    "    output, state = self.gru(x, initial_state = hidden)\n",
    "    return output, state\n",
    "\n",
    "  def initialize_hidden_state(self, batch_size):\n",
    "    return tf.zeros((batch_size, ENCODER_SIZE))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yJ_B3mhW3jFk"
   },
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "  def __init__(self, vocab_size):\n",
    "    super(Decoder, self).__init__()\n",
    "    self.vocab_size = vocab_size\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, EMBEDDING_DIM)\n",
    "    self.gru = gru(DECODER_SIZE)\n",
    "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    # used for attention\n",
    "    self.W1 = tf.keras.layers.Dense(DECODER_SIZE)\n",
    "    self.W2 = tf.keras.layers.Dense(DECODER_SIZE)\n",
    "    self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "  def call(self, x, hidden, enc_output):\n",
    "    # enc_output shape == (batch_size, max_length, hidden_size)\n",
    "\n",
    "    # hidden shape == (batch_size, hidden size)\n",
    "    # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "    # we are doing this to perform addition to calculate the score\n",
    "    hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "\n",
    "    # score shape == (batch_size, max_length, 1)\n",
    "    # we get 1 at the last axis because we are applying tanh(FC(EO) + FC(H)) to self.V\n",
    "    score = self.V(tf.nn.tanh(self.W1(enc_output) + self.W2(hidden_with_time_axis)))\n",
    "\n",
    "    # attention_weights shape == (batch_size, max_length, 1)\n",
    "    attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "    # context_vector shape after sum == (batch_size, hidden_size)\n",
    "    context_vector = attention_weights * enc_output\n",
    "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "    # x shape after passing through embedding == (batch_size, 1, EMBEDDING_DIM)\n",
    "    x = self.embedding(x)\n",
    "\n",
    "    # x shape after concatenation == (batch_size, 1, EMBEDDING_DIM + hidden_size)\n",
    "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "    # passing the concatenated vector to the GRU\n",
    "    output, state = self.gru(x)\n",
    "\n",
    "    # output shape == (batch_size * 1, hidden_size)\n",
    "    output = tf.reshape(output, (-1, output.shape[2]))\n",
    "\n",
    "    # output shape == (batch_size * 1, vocab)\n",
    "    x = self.fc(output)\n",
    "\n",
    "    return x, state, attention_weights\n",
    "\n",
    "  def initialize_hidden_state(self, batch_size):\n",
    "    return tf.zeros((batch_size, DECODER_SIZE))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a translate function\n",
    "\n",
    "1. Pass the *input* through the *encoder* which return *encoder output* and the *encoder hidden state*.\n",
    "2. The encoder output, encoder hidden state and the decoder initial state (i.e. the &lt;START&gt; token) is passed to the decoder.\n",
    "3. The decoder returns the *predictions* and the *decoder hidden state*.\n",
    "4. The next token is then fed back into the decoder repeatedly. This has two different behaviors under training and inference:\n",
    "  - during training, we use *teacher forcing*, where the correct next word is fed into the decoder, regardless of what the decoder emitted.\n",
    "  - during inference, we use `tf.argmax` to select the most likely continuation and feed it back into the decoder. This repeats until either the decoder emits an &lt;END&gt; token, indicating that it's done translating, or we run into a hardcoded length limit. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NmtTranslator(tf.keras.Model):\n",
    "  def __init__(self, encoder, decoder, start_token_id):\n",
    "    super().__init__()\n",
    "    self.encoder = encoder\n",
    "    self.decoder = decoder\n",
    "    # Uses start_token_id to initialize the decoder. (The token ID should\n",
    "    # match the decoder's language.)\n",
    "    self.start_token_id = start_token_id\n",
    "\n",
    "  def call(self, inp, targ=None):\n",
    "    '''Translate an input.\n",
    "\n",
    "    If targ is provided, teacher forcing is used to generate the translation.\n",
    "    '''\n",
    "    batch_size = inp.shape[0]\n",
    "    hidden = self.encoder.initialize_hidden_state(batch_size)\n",
    "    enc_output, enc_hidden = self.encoder(inp, hidden)\n",
    "    dec_hidden = enc_hidden\n",
    "\n",
    "    if targ is not None:\n",
    "      output_length = targ.shape[1]\n",
    "    else:\n",
    "      output_length = MAX_OUTPUT_LENGTH\n",
    "    predictions_array = tf.TensorArray(tf.float32, output_length)\n",
    "    attention_array = tf.TensorArray(tf.float32, output_length)\n",
    "    # Initialize predictions array with <START> token\n",
    "    predictions = tf.one_hot([self.start_token_id] * batch_size, decoder.vocab_size)\n",
    "    predictions_array = predictions_array.write(0, predictions)\n",
    "    attention_array = attention_array.write(0, tf.zeros((batch_size, enc_output.shape[1], 1)))\n",
    "    for i in tf.range(1, output_length):\n",
    "      if targ is not None:\n",
    "        # if target is known, use teacher forcing\n",
    "        dec_input = targ[:, i - 1]\n",
    "      else:\n",
    "        # Otherwise, pick the most likely continuation\n",
    "        dec_input = tf.argmax(predictions, axis=1)\n",
    "      # Keras RNNs wants an array instead of one time step. Add a time dimension.\n",
    "      dec_input = tf.expand_dims(dec_input, 1)\n",
    "      # passing enc_output to the decoder\n",
    "      predictions, dec_hidden, attention_weights = self.decoder(dec_input, dec_hidden, enc_output)\n",
    "      # TODO: Mask predictions so that if <END> is emitted, no more further outputs are recorded\n",
    "      # TODO: Check if all sequences have reached <END>, stop running decoder.\n",
    "      predictions_array = predictions_array.write(i, predictions)\n",
    "      attention_array = attention_array.write(i, attention_weights)\n",
    "\n",
    "    # Transpose from [time, batch, predictions] -> [batch, time, predictions]\n",
    "    return tf.transpose(predictions_array.stack(), [1, 0, 2]), tf.transpose(attention_array.stack(), [1, 0, 2, 3])\n",
    "    \n",
    "  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P5UY8wko3jFp"
   },
   "outputs": [],
   "source": [
    "encoder = Encoder(len(english_index))\n",
    "decoder = Decoder(len(spanish_index))\n",
    "start_token_id = spanish_index.word2idx[START_TOKEN]\n",
    "model = NmtTranslator(encoder, decoder, start_token_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_ch_71VbIRfK"
   },
   "source": [
    "## Define the optimizer and the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WmTHr5iV3jFr"
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer()\n",
    "\n",
    "def loss_fn(real, pred):\n",
    "  # The target output is a batch of sentences of varying length,\n",
    "  # and the prediction is a batch of sentences of varying length,\n",
    "  # which aren't necessarily the same length as the target output.\n",
    "\n",
    "  # Loss functions like BLEU try to handle this, but we'll hack a simpler one here.\n",
    "\n",
    "  # First, cut down the prediction to the correct shape\n",
    "  pred = pred[:, :real.shape[1], :]\n",
    "  # then mask the prediction so that we're comparing word-for-word with\n",
    "  # the true answer, and ignoring any extra words\n",
    "  # i.e. \n",
    "  # ['This', 'is', 'the', 'correct', 'answer', '.', '<end>', '<padding>', '<padding>', '<padding>']\n",
    "  # ['This', 'is', 'what', 'the', 'model', 'emitted', 'blah', 'blah', '.', '<end>']\n",
    "  # results in comparing\n",
    "  # This/This, is/is, the/what, correct/the, answer/model, ./emitted, <end>/blah, and ignoring the rest.\n",
    "  mask = 1 - (real == 0)\n",
    "  loss_ = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=real, logits=pred) * mask\n",
    "  return tf.reduce_mean(loss_)\n",
    "\n",
    "# Create a step variable to track how much we've trained.\n",
    "global_step = tf.Variable(0, dtype=tf.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, dataset, global_step):\n",
    "  \"\"\"Trains model on `dataset` using `optimizer`.\"\"\"\n",
    "  start = time.time()\n",
    "  avg_loss = tf.metrics.Mean('loss', dtype=tf.float32)\n",
    "  for (batch, (inp, targ)) in enumerate(dataset):\n",
    "    with tf.GradientTape() as tape:\n",
    "      predictions, _ = model(inp, targ=targ)\n",
    "      loss = loss_fn(targ, predictions)\n",
    "\n",
    "    avg_loss(loss)\n",
    "    gradients = tape.gradient(loss, model.variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.variables))\n",
    "    if batch % 10 == 0:\n",
    "      summary_ops_v2.scalar('loss', avg_loss.result(), step=global_step)\n",
    "      avg_loss.reset_states()\n",
    "      rate = 10 / (time.time() - start)\n",
    "      print('Step #%d\\tLoss: %.6f (%.2f steps/sec)' % (batch, loss, rate))\n",
    "      start = time.time()\n",
    "      break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, dataset, global_step):\n",
    "  \"\"\"Perform an evaluation of `model` on the examples from `dataset`.\"\"\"\n",
    "  avg_loss = tf.metrics.Mean('loss', dtype=tf.float32)\n",
    "  for (batch, (inp, targ)) in enumerate(dataset):\n",
    "    predictions, _ = model(inp)\n",
    "    loss = loss_fn(targ, predictions)\n",
    "    avg_loss(loss)\n",
    "    break\n",
    "\n",
    "  print('Model test set loss: {:0.4f}'.format(avg_loss.result()))\n",
    "  summary_ops_v2.scalar('loss', avg_loss.result(), step=global_step)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DMVWzzsfNl4e"
   },
   "source": [
    "## Configure model directory\n",
    "\n",
    "We'll use one directory to save all of our relevant artifacts (summary logs, checkpoints, SavedModel exports, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zj8bXQTgNwrF"
   },
   "outputs": [],
   "source": [
    "# Where to save checkpoints, tensorboard summaries, etc.\n",
    "MODEL_DIR = '/tmp/tensorflow/nmt_attention'\n",
    "\n",
    "def apply_clean():\n",
    "  if tf.io.gfile.exists(MODEL_DIR):\n",
    "    print('Removing existing model dir: {}'.format(MODEL_DIR))\n",
    "    tf.io.gfile.rmtree(MODEL_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "text",
    "id": "hpObfY22IddU"
   },
   "outputs": [],
   "source": [
    "# Optional: remove directory\n",
    "apply_clean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_summary_writer = tf.summary.create_file_writer(\n",
    "  os.path.join(MODEL_DIR, 'summaries', 'train'), flush_millis=10000)\n",
    "test_summary_writer = tf.summary.create_file_writer(\n",
    "  os.path.join(MODEL_DIR, 'summaries', 'eval'), flush_millis=10000, name='test')\n",
    "\n",
    "checkpoint_dir = os.path.join(MODEL_DIR, 'checkpoints')\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt')\n",
    "checkpoint = tf.train.Checkpoint(\n",
    "    encoder=encoder, decoder=decoder, optimizer=optimizer,\n",
    "    global_step=global_step)\n",
    "# Restore variables on creation if a checkpoint exists.\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ddefjBMa3jF0",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "NUM_TRAIN_EPOCHS = 10\n",
    "for i in range(NUM_TRAIN_EPOCHS):\n",
    "  start = time.time()\n",
    "  with train_summary_writer.as_default():\n",
    "    train(model, optimizer, train_ds, global_step)\n",
    "  end = time.time()\n",
    "  print('\\nTrain time for epoch #{} ({} total steps): {}'.format(\n",
    "      i + 1, global_step.numpy(), end - start))\n",
    "  with test_summary_writer.as_default():\n",
    "    test(model, test_ds, global_step)\n",
    "  checkpoint.save(checkpoint_prefix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for plotting the attention weights\n",
    "def plot_attention(attention, sentence, predicted_sentence):\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attention, cmap='viridis')\n",
    "    \n",
    "    fontdict = {'fontsize': 14}\n",
    "    \n",
    "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
    "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def translate_and_plot(model, sentence, english_index, spanish_index):\n",
    "    \"\"\"Run translation on a sentence and plot an attention matrix.\n",
    "    \n",
    "    Sentence should be passed in as list of tokenized words.\n",
    "    \"\"\"\n",
    "    english_ints = tf.constant([[english_index.word2idx[word] for word in sentence]])\n",
    "    predictions, attention = model(sentence)\n",
    "    prediction_ids = tf.squeeze(tf.argmax(predictions, axis=-1))\n",
    "    attention = tf.squeeze(attention)\n",
    "    predicted_sentence = [spanish_index.idx2word[id.numpy()] for id in prediction_ids]\n",
    "    print('Input: {}'.format(' '.join(sentence)))\n",
    "    print('Predicted translation: {}'.format(' '.join(sentence)))\n",
    "    plot_attention(attention, sentence, predicted_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "translate_and_plot(model, train_sentence_pair[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n250XbnjOaqP"
   },
   "source": [
    "## Restore the latest checkpoint and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UJpT9D5_OgP6"
   },
   "outputs": [],
   "source": [
    "# restoring the latest checkpoint in checkpoint_dir\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WrAM0FDomq3E"
   },
   "outputs": [],
   "source": [
    "translate('hace mucho frio aqui.', encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zSx2iM36EZQZ"
   },
   "outputs": [],
   "source": [
    "translate('esta es mi vida.', encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A3LLCx3ZE0Ls"
   },
   "outputs": [],
   "source": [
    "translate('¿todavia estan en casa?', encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DUQVLVqUE1YW"
   },
   "outputs": [],
   "source": [
    "# wrong translation\n",
    "translate('trata de averiguarlo.', encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RTe5P5ioMJwN"
   },
   "source": [
    "## Next steps\n",
    "\n",
    "* [Download a different dataset](http://www.manythings.org/anki/) to experiment with translations, for example, English to German, or English to French.\n",
    "* Experiment with training on a larger dataset, or using more epochs\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "nmt_with_attention.ipynb",
   "private_outputs": true,
   "provenance": [
    {
     "file_id": "1C4fpM7_7IL8ZzF7Gc5abywqQjeQNS2-U",
     "timestamp": 1527858391290
    },
    {
     "file_id": "1pExo6aUuw0S6MISFWoinfJv0Ftm9V4qv",
     "timestamp": 1527776041613
    }
   ],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
